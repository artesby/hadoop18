{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Мотивация \n",
    "- Написание Hadoop-задач на Java нетривиально\n",
    "- Хочется использовать возможность Hadoop для обработки слишком больших для обычной реляционной БД данных\n",
    "- Хочется иметь определенную схему данных\n",
    "- SQL? Да - HiveSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Архитектура"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cwiki.apache.org/confluence/download/attachments/27362072/system_architecture.png)\n",
    "\n",
    "- `Metastore` хранит метаинформацию о таблицах, колонках и их типах, где и как они хранятся и т.д.\n",
    "    - В качестве хранилища может использоваться реляционная БД\n",
    "- `Driver`- компонент, который обслуживает жизненный цикл запроса к `Hive`.\n",
    "- `Query Compiler` - компонент, который обрабатывает запроса на `HiveSQL` и преобразует его в последовательность MapReduce-задач\n",
    "- `Execution Engine` - компонент, который непосредственно запускает MapReduce-задачи. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Клиент Python\n",
    "\n",
    "\n",
    "Под `Windows` используйте рецепт отсюда https://stackoverflow.com/a/51023150 . Под `Linux` установите `libsasl`:\n",
    "\n",
    "```bash \n",
    "sudo apt install libsasl2-dev\n",
    "```\n",
    "\n",
    "Затем:\n",
    "```bash\n",
    "python3 -m pip install --user -U PyHive[hive]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=['*org.apache.hive.service.cli.HiveSQLException:Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask:28:27', 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:316', 'org.apache.hive.service.cli.operation.SQLOperation:runQuery:SQLOperation.java:156', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:183', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:257', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:388', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:369', 'sun.reflect.GeneratedMethodAccessor12:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:-2', 'javax.security.auth.Subject:doAs:Subject.java:422', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1698', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'com.sun.proxy.$Proxy19:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:262', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:488', 'org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1313', 'org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1298', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:39', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:285', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1149', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:624', 'java.lang.Thread:run:Thread.java:748'], sqlState='08S01', errorCode=2, errorMessage='Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask'), operationHandle=None)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-54d6ea27751c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         cursor.execute('''\n\u001b[1;32m     12\u001b[0m             \u001b[0mINSERT\u001b[0m \u001b[0mINTO\u001b[0m \u001b[0mTABLE\u001b[0m \u001b[0mperson\u001b[0m \u001b[0mVALUES\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"John\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Mike\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         ''')\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyhive/hive.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, operation, parameters, **kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecuteStatement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0m_check_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_operationHandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperationHandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyhive/hive.py\u001b[0m in \u001b[0;36m_check_status\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatusCode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mttypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTStatusCode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUCCESS_STATUS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOperationalError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=['*org.apache.hive.service.cli.HiveSQLException:Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask:28:27', 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:316', 'org.apache.hive.service.cli.operation.SQLOperation:runQuery:SQLOperation.java:156', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:183', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:257', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:388', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:369', 'sun.reflect.GeneratedMethodAccessor12:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:-2', 'javax.security.auth.Subject:doAs:Subject.java:422', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1698', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'com.sun.proxy.$Proxy19:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:262', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:488', 'org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1313', 'org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1298', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:39', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:285', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1149', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:624', 'java.lang.Thread:run:Thread.java:748'], sqlState='08S01', errorCode=2, errorMessage='Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask'), operationHandle=None)"
     ]
    }
   ],
   "source": [
    "from pyhive import hive  \n",
    "with hive.connect('localhost', username='root') as conn:\n",
    "    with conn.cursor() as cursor:        \n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS person (               \n",
    "                           n STRING, \n",
    "                           age INT)\n",
    "            ''')\n",
    "        \n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('''\n",
    "            INSERT INTO TABLE person VALUES (\"John\", 25), (\"Mike\", 50)\n",
    "        ''')\n",
    "     \n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('''\n",
    "            SELECT * FROM person\n",
    "        ''')\n",
    "        \n",
    "        print(cursor.fetchall())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Хранение данных\n",
    "\n",
    "- База Данных - это набор Hive-таблиц\n",
    "    - физически представляет собой директорию на HDFS\n",
    "- Таблицы в базе данных \n",
    "    - таблицы физически хранятся в поддиректориях\n",
    "    - метаинформация о таблице хранится в `Metastore`\n",
    "\n",
    "Создать базу дынных:\n",
    "```sql \n",
    "CREATE DATABASE test_db;\n",
    "```\n",
    "   \n",
    "Создать таблицу:\n",
    "```sql\n",
    "CREATE TABLE weather ( \n",
    "               dt TIMESTAMP, \n",
    "               t FLOAT, \n",
    "               po FLOAT,\n",
    "               p FLOAT, \n",
    "               u FLOAT, \n",
    "               vv FLOAT, \n",
    "               td float, \n",
    "               n STRING) \n",
    "           ROW FORMAT DELIMITED FIELDS TERMINATED BY ','; \n",
    "               \n",
    "LOAD DATA LOCAL INPATH '/course/data/weather_stat.csv' INTO TABLE weather;               \n",
    "```\n",
    "\n",
    "\n",
    "```sql\n",
    "CREATE TABLE cik ( \n",
    "               region STRING, \n",
    "               tik STRING, \n",
    "               uik STRING, \n",
    "               voters_total INT,\n",
    "               total INT, \n",
    "               total_ahead INT,\n",
    "               total_inside INT, \n",
    "               total_outside INT, \n",
    "               total_removed INT,\n",
    "               outside INT,\n",
    "               inside INT, \n",
    "               invalid INT,\n",
    "               valid INT, \n",
    "               lost INT,\n",
    "               unkwn INT,\n",
    "    \n",
    "               baburin INT,\n",
    "               grudinin INT, \n",
    "               zhirinovsky INT, \n",
    "               putin INT, \n",
    "               sobchak INT,\n",
    "               suraykin INT,\n",
    "               titov INT, \n",
    "               yavlinsky INT\n",
    ") ROW FORMAT DELIMITED FIELDS TERMINATED BY ','; \n",
    "               \n",
    "LOAD DATA LOCAL INPATH '/course/data/cik_trunc.csv' INTO TABLE cik;               \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Типы данных\n",
    "\n",
    "##### Numeric Types\n",
    "\n",
    "    TINYINT (1-byte signed integer)\n",
    "    SMALLINT (2-byte signed integer)\n",
    "    INT/INTEGER (4-byte signed integer)\n",
    "    BIGINT (8-byte signed integer)\n",
    "    FLOAT (4-byte single precision floating point number)\n",
    "    DOUBLE (8-byte double precision floating point number)\n",
    "    DECIMAL\n",
    "    NUMERIC \n",
    "\n",
    "##### Date/Time Types\n",
    "\n",
    "    TIMESTAMP \n",
    "    DATE \n",
    "    INTERVAL\n",
    "\n",
    "##### String Types\n",
    "\n",
    "    STRING\n",
    "    VARCHAR \n",
    "    CHAR \n",
    "\n",
    "##### Misc Types\n",
    "\n",
    "    BOOLEAN\n",
    "    BINARY \n",
    "\n",
    "##### Complex Types\n",
    "    \n",
    "    arrays: ARRAY<data_type> \n",
    "    maps: MAP<primitive_type, data_type> \n",
    "    structs: STRUCT<col_name : data_type [COMMENT col_comment], ...>\n",
    "    union: UNIONTYPE<data_type, data_type, ...> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание таблиц\n",
    "\n",
    "Колонки можно разделять с помощью регулярных выражений\n",
    "\n",
    "```sql\n",
    "CREATE TABLE apachelog (\n",
    "      host STRING,\n",
    "      identity STRING,\n",
    "      user STRING,\n",
    "      time STRING,\n",
    "      request STRING,\n",
    "      status STRING,\n",
    "      size STRING,\n",
    "      referer STRING,\n",
    "      agent STRING)\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'\n",
    "WITH SERDEPROPERTIES (\n",
    "  \"input.regex\" = \"([^]*) ([^]*) ([^]*) (-|\\\\[^\\\\]*\\\\]) ([^ \\\"]*|\\\"[^\\\"]*\\\") (-|[0-9]*) (-|[0-9]*)(?: ([^ \\\"]*|\\\".*\\\") ([^ \\\"]*|\\\".*\\\"))?\"\n",
    ")\n",
    "STORED AS TEXTFILE;\n",
    "```\n",
    "\n",
    "Для кастомного CSV\n",
    "\n",
    "```sql\n",
    "CREATE TABLE my_table(a string, b string, ...)\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "WITH SERDEPROPERTIES (\n",
    "   \"separatorChar\" = \"\\t\",\n",
    "   \"quoteChar\"     = \"'\",\n",
    "   \"escapeChar\"    = \"\\\\\"\n",
    ")  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning\n",
    "\n",
    "Физически партиции хранятся в разных директориях\n",
    "\n",
    "```sql \n",
    "\n",
    "CREATE TABLE page_view(viewTime INT, userid BIGINT,\n",
    "     page_url STRING, referrer_url STRING,\n",
    "     ip STRING COMMENT 'IP Address of the User')\n",
    " COMMENT 'This is the page view table'\n",
    " PARTITIONED BY(dt STRING, country STRING)\n",
    " STORED AS SEQUENCEFILE;\n",
    "```\n",
    "\n",
    "Можно использовать динамическое партиционирование \n",
    "\n",
    "```sql\n",
    "SET hive.exec.dynamic.partition.mode=nonstrict;\n",
    "\n",
    "CREATE TABLE cik_ext(\n",
    "    uik string, \n",
    "    valid INT, \n",
    "    putin INT, \n",
    "    grudinin INT) \n",
    " partitioned BY (region string);\n",
    " \n",
    "INSERT overwrite TABLE cik_ext partition(region) \n",
    "SELECT region, uik \n",
    "FROM cik\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skewed tables\n",
    "\n",
    "```sql\n",
    "CREATE TABLE skewed_table (col1 STRING, col2 INT, col3 STRING)\n",
    "  SKEWED BY (col1, col2) ON (('s1',1), ('s3',3), ('s13',13), ('s78',78)) STORED AS DIRECTORIES;\n",
    "```                                                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucketed table\n",
    "\n",
    "```sql\n",
    "CREATE TABLE page_view(\n",
    "    viewTime INT, \n",
    "    userid BIGINT,\n",
    "    page_url STRING, \n",
    "    referrer_url STRING,\n",
    "    p STRING COMMENT) \n",
    " PARTITIONED BY(dt STRING, country STRING)\n",
    " CLUSTERED BY(userid) SORTED BY(viewTime) INTO 32 BUCKETS\n",
    " STORED AS SEQUENCEFILE;\n",
    " ```\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
